# Intro to Cs and our Computer

Here I assume you have no, or very little knowledge of what Computer Science is. I also assume that if you had to describe what a computer is, you would hestiate momentarily before spouting out sometype of nonsense over how the internet is made up of a series of tubes, and that the monitor is actually the computer as that is where the visuals come out of. Which there is nothing wrong with at all. If you are pass that point in your learning, maybe still read through this, as you never know what you may learn.

So with this out of the way, let us go ahead and start with our journey!

## What is Computer Science

The first question we should have is pretty simple, what is computer science? It is such a fundamental question that so many people have trouble answering, and naturally is our starting position. So let us define what exactly computer science really is. At the highest level, we can view computer science as the process of solving a problem. Now obviously this definition abstracts away a lot of what makes computer science what it is, but it is a good stepping stone that we can build up as time goes by. To build on this a little more, we can further denote that computer science involves the process of taking some type of input, and from that input, generating some type of output. There, now that definition has a little bit more meat on it.

By taking this new definition, we can break it down into three steps:

1.  Input for the problem
2.  The Black Box (Where the computer comes into play as well as the algorithm)
3.  The output produced from the Black Box

At the most basic level, this is how we can view computer science. Just a way that we take our inputs for our problem, manipulate it in some way through an algorithm on our computer, and then produce an output with the intention of solving our initial problem. Don't worry about the term Black Box. For now, when you encounter it, just think of your computer, and how you don't truly understand what is going on inside of it, but you do know that it is responsible for generating your output.

Pretty much every computer science problem can be boiled down into this. Furthermore, there is a reason why we do not explicitly mention a computer in the second step. First, it would probably be more apt to just call it an algorithm instead, as that is what we really will be focusing on. Second, despite computer being in the name, computer science isn't really the study of the computer itself. Instead, we are focusing on the study of software and software systems. As you can tell, maybe the above definition isn't has encompassing as we might have though. Perhaps it is even purposely a little misleading. How about we build on it a little more with a slightly more complex definition.

So keeping the original definition we gave for computer science, we can say that computer science is the study of information, protocols, and algorithms for idealized and real automata. Pretty simple, and at this point if you don't understand that, I'm not sure if I can help you. Just kidding. I had no idea in hell what that meant when I first read it myself years back. So for the sake of our sanity, let us break it down into digeastible parts:

* Information
    * Knowledge that is represented in a form that is suitable for transmission and manipulation
    * THis is obviously step 1 of our previous definition, as this would be our inout data.
* Protocol
    * Rules for exchanging information without problems.
    * This is the bridge between step 1 and step 2 of our previous definition.
    * We will be breaking this down further once we tackle binery, but it deals with making our input something that we can process.
* Algorithm
    * An algorithm is an unambiquous, finite description in simple steps or actions.
    * This is our black box, or our second step of our original definition.
    * This is a very essential concept within computer science, and one that we will be talking about in great detail.
* Automata/Automation
    * This is probably the one that you will have the hardest time wrapping your head around.
    * Automata and automation is a self deciding or autonomous mechanism with bounded resources (which could be time and/or space).
    * We can view this as the process of our computer or program moving through a series of states.
    * This idea is a very important one, which may be hard to wrap your head around at this moment.

One thing you should keep in mind is that computer science is a very broad field, like most other disciplines. You can view it as this, just because you are a lawyer, doesn't mean that you practice both family law and corporate law. I'm not even sure if those are both true law fields, but all I am saying is that I am not hiring a divorce lawyer to help with my anti-trust case. Within computer science, there are also many different disciplines, though they tend to be much more closely connected than my previous, terrible, analogy. The way I view it is that it is very helpful to have an understanding of many of the different disciplines within computer science, but you should really only master a few.

So let's look at just a few of these disciplines:

* Artificial Intelligence
* Computer-Human Interface / UX UI
* Game Design
* NetWorks
* Computer Graphics
* Information Security
* Data Science
* Software Engineering
* Systems Engineering

Now these are far from all encompassing, and within these disciplines, there are further sub-disciplines, but the aim of this is to give you some things for you to research yourself.

One thing is that you really do not have to choose a discipline right now. You can just follow through with this, learning whatever you want, and choose one later one when you are working professionally. There really isn't any reason why you can't move between disciplines, as long as you have a good foundation in knowledge. That foundation is what I am really hoping to depart to you.

## What is a Computer

So we have talked long enough about the basics of what computer science is. Now I want to touch on the machine you are using right now. Despite what I said before, about computer science not really being about the study of the actual computer, it is still important to have an understanding of what it is.

I get it, though, you probably know what a computer is. Especially at a high level. I am not here to insult your intelligence, or basic understanding of technology. Instead I want to try to setup the idea of a computer in the context of computer science itself.

So let's define our computer as a machine whose purpose is to perform calculations and remember the results of said calculations. Viewing the computer through this lens will make it generic enough to encompass a bunch of different types of "computers", as well as having it fit nicely into our definition for computer science. It also allows to compare the computer against humans. It is this comparison that truly shows the power of the computer. See any person can perform calculations and remember the results of those calculations, but it is the power and efficiency of the computer which makes it stand out.

A computer has the ability to perform these calculations billions of times per second, and has the capability of holding tetrabytes of information. I'm not sure how much information a human brain can hold, though I do bet it is a lot, but I doubt it can stand toe to toe with a computer. Ontop of this, we have manufacturers who are constantly pushing the limits of what we know a computer is capable of. If you need proof, just look at a mid-range computer even 10 years ago. Computers now, at the same price range, can vastly out perform. Now take an average kid from 10 years ago and compare to a kid now. The only difference is that they probably aren't on facebook.

Despite all of this, there are still problems that are simply unattainable, at least within a reasonable time, which a computer cannot feasibly solve. Irregardless of this, computers are still able to push the limits of what humans are able to achieve.

So much like how our definition of computer science evolved, let's evolve our definition of the computer. Now we want to say that a computer is a programmable machine which can perform both arithmetic, as well as logical, operations in an automatic and sequential way. It also has the ability to store, process, and receive data.

To wrap this snuggly into our definition of computer science, we can say that at its most base level, programming is just about how we store and process data. With this, we can also say that a computer is just a data processor. Furthermore, we can break the idea of a data processor down into three core parts:

* Input of Data
* Manipulation/Processing of Data
* Giving Output

This should really look familar, as it is our definition of what computer science is. Though a computer now does much more than just simple data processing, the intial computers were built for this express purpose. To be honest, I would even argue that most, if not all, processes involve some form of data processing, even if in the most abstract of ways.

So now that we have a working model of what a computer is, we want to also define its major functions as well:

* A computer accepts the command and/or data as inputs given by the user.
* A computer follows the instructions and stores the data given by the user.
* A computer processes data as per the instructions given by the user.
* A computer gives the desirable results in the form of outputs.

At this point, we should have a pretty decent idea of what a computer is theoretically, but why should we choose to use a computer when we want to solve a problem? Though this is probably self-evident, let's list out the reasons in as academic of a way as possible:

### Automation

The computer, as well as the operating system of a computer, is largely automatic in nature. This means that it does not actually need intervention iin performing its tasks. This means, in short, that we are able to give it commands, and it will perform the work automatically.

### Speed

A computer has the ability to take millions of instructions per second. This is much faster than a human has the ability to.

### Storage

A computer has the ability to store an enourmous amount of data, and in many different formats. Further, you will see the storage (space) on a computer expressed as Kilobytes (KB), Megabytes (MB), Gigabytes (GB) and Terabytes (TB).

### Accuracy

One thing about a computer is that its accuracy is very high. So high that most issues with accuracy that you run into will be less an issue with accuracy from the computer, and more with the human that programmed it.

### Versatility

We can perform a huge amount of different tasks on a computer. As time goes on, the amount of tasks we are able to perform increases.

### Diligence

Your computer will never get tired; mostly because it is a machine. Though you can view your computer overheating like having a heat stroke.

### Reliability

A computer will always give you what you ask, either for good or bad. This means that it is very reliable. This also means that if you didn't get the answer you were looking for, you probably were not asking the right question.

### Vast Memory

A computer system can have a wide range of memory, allowing you to recall the data at any time. One point I want to bring up, when we talk about memory here, we mean something completely different than storage. We will break this down further later.

## Computer Systems and You

Now that we are essentially experts on all things computers and computer science, we need to ask ourselves another important question: How do we actually use a computer? Obviously we can't just grab a computer and expect it to actually do something. A computer relies on multiple components to be usable by the user. These components make up the actual computer system itself. We can even break these components down into two aspects, the hardware, and the software.

### Hardware

The hardware of a computer consists of the actual physical components of a computer system. We can say that these are the parts of the computer you can physically see yourself. These can be broken down into the following:

* Monitor
    * What we display our visual results to.
* CPU
    * The central processing unit that controls the computer's functions, as well as transmits our data.
* Motherboard
    * Used to establish the communication between components and transmission of information.
* RAM
    * Random Access Memory, which is responsible for the storage of programs that are currently running, and also stores our data temporarily.
* Hard Drive
    * The permanent memory storage drive.
* GPU
    * The graphical processing unit responsible for handling intensive graphics rendering tasks.

Now this is a pretty high level view of the hardware, but will suffice for now. Next we want to look at the second major component of a computer system.

### Software

The above hardware components are nice and all, but without software, we wouldn't actually be able to utilize them in any meaningful way. Let's go ahead and give us a simple definition of what software is before we break it up into its three main types. Software is a program that performs different commands that we, as the user, give to it. The three main types of software are:

*  Operating System
      *  The operating system helps us load the basic program automatically as soon as the computer is started.
      *  The following three are the main operating systems that you will see:
            *  Microsoft Windows
            *  Max OS x
            *  Linux
*  Application Software
      *  This is the software that can be used on your installed os.
      *  Some of these are:
            *  Office Programs
            *  Web Browsers
            *  Antivirus Programs
*  E-Accessibility Software
      *  These are software that grant additional uses to a user:
            *  Screen Readers
            *  Video Games
            *  Learning Software

This is about the depth we are going to go into these at this moment, but throughout our journey, we are going to dive into not only all three types of components, but also the hardware as well. Though someone of these will be on the branching course paths, therefore optional to you.

## Computational Thinking and Algorithms

Now that we have some of the basics out of the way, we can finally look at what you may think of as actually in the domain of computer science (the above is as well, but you'll see what I mean in a second). For this, we want to tackle in two parts. The first is computational thinking, and the second is algorithms. For many people, these two topics are really their first insight into computer science, and a integral part into their journey of being more than just a programmer, but a software engineer.

So before I rant on any longer, we can break computational thinking down into the idea of knowledge. More importantly, declarative knowledge and imperative knowledge. Maybe forbidden knowledge, but that may come more into play later. Let's break out these two types of logic though:

### Declarative Knowledge

We can view declarative knowledge as being composed of statements of facts. An example of this can be the following function for a derivative:

$f'(x)=\lim_{h\rArr0}\frac{f(x+h)-f(x)}{h}$

Despite the fact that the above is unequivocally true, it really doesn't tell us much. If we were given this with no prior knowledge, we might struggle figuring out what we were looking at. But we that doesn't change that this is a statement of fact.

### Imperative Knowledge

Instead, we want to take the above function and break it out into a series of steps to solve it. We call the knowledge of how we solve this statement of fact imperative knowledge. A common way to view this within computer science is that imperative knowledge is like a recipe, a series of steps to follow.

So let's break down the above now into a series of steps:

1.  Substitute

$f(x+h)=\sqrt{x+h}$

and

$f(x)=\sqrt{x}$

into

$f'(x)=\lim_{h\rArr0}\frac{f(x+h)-f(x)}{h}$

which gives us

$f'(x)=\lim_{h\rArr0}\frac{\sqrt{x+h}-\sqrt{x}}{h}$

2.  Multiply the numerator and denominator by

$\sqrt{x+h}+\sqrt{x}$

without distributing in the denominator

$=\lim_{h\rArr0}\frac{\sqrt{x+h}-\sqrt{x}}{h}\times\frac{\sqrt{x+h}+\sqrt{x}}{\sqrt{x+h}+\sqrt{x}}$

3. Muliply the numerators and simplify

$=\lim_{h\rArr0}\frac{h}{h\left(\sqrt{x+h}+\sqrt{x}\right)}$

cancel the h

$=\lim_{h\rArr0}\frac{1}{\left(\sqrt{x+h}+\sqrt{x}\right)}$

4. Evaluate the limit

$=\dfrac{1}{2\sqrt{x}}$

Note that the description of the method is a relatively simple sequence of steps which includes a flow that controls which steps are controlled and in what order.

### Algorithms

Algorithms are the black box that we discussed earlier, or atleast to a degree. By that, I mean that for now, we only care about the output derived from this "black box", and not the internals of the actual process. Which makes sense at this point, seeing that we are only looking at the process which our given inputs are converted to our expected outputs. This is an important concept in black box testing, which we will dive into sooner than later.

Before we get ahead of ourselves, though, we want to first give a better definition to algorithms. Something that we can actually work with:

Algorithms are a finite list of instructions which aim to describe a computation, which when executed on a provided set of inputs will proceed through a set of well-defined states and eventually provide an output.

That is a decent amount to take in, so let me break it down into something slightly simpler:

An algorithm is just a set of instructions that we follow step by step to get a desired result. It is esentially a recipe.

Algorithms are an essential aspect of computer science, whether you are making embedded systems, websites, or games. It can by one of the facets that separates the engineers from the scripters, and there is a lot behind it. It is also much easier than you may expect it to be, as long as you go in with a strong knowledge of computer science in general.

Now that we have a rudimentary understanding of algorithms, let's look at how we get these into our computers. One way to do this is by designing a machine which is specifically created to compute this algorithm for us. The first ever computers fell into this category of machines. We actually call these fixed-program computers. In the case of our derivative, the computer can be aimed at just solving derivatives in an effective manner. Or perhaps calculus or math in general. Perfect example of these are calculators, which do just this.

What if we wanted our program to do more than just solve derivatives? I know, even the thought of this evokes so much heresy, but hear me out. Perhaps we want to process a word document, or a csv? What if we wanted to play a video game? For these, we would then make use of something that we call a stored-program computer.

A stored-program computer aims to store and manipulate a sequence of instructions, as well as contain a set of elements which will execute any instruction in said sequence. That is where the power of a stored-program computer comes into play. By allowing the computer to accept a sequence of instructions, we can create an extremely powerful and flexible machine.

In the computer itself, both the data and the program live within the memory. We will dive much deeper into this in time, but to keep it simple, the computer contains a counter which will update to point to the memory address that the data currently resides in. Our computations will typically start at this point in memory, and will execute the instructions before moving to the next place in the sequence sequentially.

Sometimes, though, we do not want to move sequentially through memory. We can actually add tests that can allow us to jump between non-sequential parts of our sequence. We call this the flow of control, and it is an essential part of our applications.

Now, in order for us to create our sequence of instructions, we need a language to describe it. We need some way for us to relate our orders to our machine. This is our programming language. We still aren't quite at the stage for us to start writing our first programs, or learning our first language, but we are almost there. One cool thing about our programming language is the idea of a program being Turing Complete. This means that if we write our algorithm in one program, we can translate it into another. This is going to be a really important topic for us, especially as we start learning different languages.

The last important topic over algorithms that we are going to visit here is the idea of Big O Notation. Though we are going to just focus on the extreme high level look at it, as we really do not have enough understand or programming to really dive deeper into it. Big O deals with the efficiency of our algorithm, and is broken down into time complexity, and memory complexity. Most times, though, you will care about time complexity. This especially holds true as the cost of each unit of memory falls, yet your customers expect faster and faster response times.

Now in order to actually introduce the concept to you, we will use the example of the phone book. This is a pretty typical example that you will see a lot when first introducing algorithms, so no reason changing it.

So, let's say that we have a phone book. Within that phone book, we want to search for a friend. For this particular phone book, let's say that  we do not have an index. For completeness, we will say that with the inclusion of an index, we have a hashmap, or dictionary. This obviously makes searching much faster. But instead we have a sorted array. This means that the names in the phonebook are sorted alphabetically by last name, then first name.

We could search through each page manually, and we have algorithms that do this type of linear search. It isn't the most efficient, but in certain cases, it is the only route we may have. Remember though that I mentioned our book is already sorted. This means that we can instead choose a more optimal solution. Instead, we want to make use of a divide and conquer technique. We want to open the book to the middle, check to see if the name is on that page, and either repeat to either the left hand side (names that came before), or the right hand side (names that came after). Decisions like these are what you need to consider when creating your algorithm. One algorithm may not work for every situation. We may have a sorting algorithm that runs slower than other sorting algorithms, but you still may have situations where you want to use it (embedded systems are one example). These are all things you need to consider when creating your code.

This brings us to one very important concept in the world of Big O Notation, which is the idea of scalability with your program. Though I won't be going into detail with this now (don't worry, we are going to hit this so hard, it will be burned into your brain), I still want to show you the typical representation of these scaling issues:

![](BigO.png)

This shows the relationship of operations by elements as our elements increase. Or in better words, the time it takes to solve a problem compared to the size of the problem. As we see, Ordo 1, or O(1), constant time is the best performance we can get.

In our phone book problem from before, if we were to go page to page, we would have linear time complexity. This means that our operations and elements scale linearly:

$$f(n)$$

Now if we look at our divide and conquer algorithm, our solution becomes a logarithmic solution with ordo time complexity of:

$$\log{n}$$

The main thing to keep in mind with our logarithmic solution, is that it is much more scalable even compared to our linear solution. The worse case scenario is much better, as we do not have to iterate through the whole data structure to find out that the element does not exist. The issue, though, is that if the element is at the beginning of the data structure, it isn't as efficient.

## What is a Programming Language

We keep on talking about algorithms and computers, but we have barely touched on what is probably the thing you are most looking forward to learning about, the programming language.

There are hundreds of programming languages in the world. Some are meme languages, some just never caught on, or are so niche that you will probably never use them. You have very general purpose languages, like Python or Java, and languages that are much more specific, like Octave or Matlab. You can use Python for data science, and many do, but Matlab or Octave are more optimized towards this.

So what makes a language a language? Every language has a set of primitive constructs, syntax, static semantics, and semantics that define what the language is. So let's break out each of these:

### Primitive Constructs

These are the actual workds of a language. In Python, for example, we have literals (10, 5.2, 'a'), as well as infix operators (+, -, *).

### Syntax

This defines which strings of characters and symbols are well formed. In Python, ```5.0 / 5.0``` is well formed, but ```5.0 5.0``` is not well formed.

### Static Semantics

These define which syntactically valid strings have a meaning to them. Within Python, ```"Error" / 404``` is syntactically well formed ```(<literal/><operator/><literal/>)```, but would cause an error because you cannot divide the string ```Error``` by ```404```.

### Semantics

Semantics in a language associates a meaning with each syntactically correct string of symbols which contain no static semantic errors. Unlike other natural languages, computer languages are made so that each legal program has only one meaning. We will look into errors in more detail at a later time, once we have some more programming knowledge under our belts. One thing to keep in mind, though, is that when a computer program does have an error within it, the results can be unpredictable at times.

So we can see here that each aspect of a language builds upon each other. This is a very important concept to remember. But this brings us to the next major aspect of a language that we need to talk about, Natural vs. Formal languages.

### Natural Languages

A natural language is what you think of most when you imagine a language. This is the language that people actually speak. In my case here, English. Every time I talk to someone else, I am using a natural langauge to relay information. These are languages that have evolved over time. They weren't designed by people, or atleast not in the same way that a computer language was. There is nuance to a natural language, and one word or phrase may have multiple meanings.

One example I use to try to differentiate a natural language from a formal one would be the city I live in, Atlanta. Atlanta doesn't always make sense. We have multiple Peachtree streets. Roads that randomly merge with other roads for no reason. Sometimes it feels like a blind drunkard designed the city. The reason for this is because Atlanta evolved naturally over time. Though year it was still built and planned by people, it is more like a chaotic mess that we have tried to impose order to. This is much like a natural language.

### Formal Languages

These languages are designed by people for a specific application or domain. This is the category that programming languages fall into, but other applications fall into this as well. Mathematical formulaes are a formal language. If you have ever taken a physics or chemistry course, or even a econ course, you know that they also have their own formal languages.

Specifically, a programming language is a formal language that has been designed to express computations. We can see the specific applications that a programming language is used for. This means that there is no ambiquity to it.

If we bring back the city analogy, we can view a formal langauge like a military base. Military bases are deliberately and purposefully planned, typically in a grid fashion. You won't randomly see a theme park in the middle of the base, as there are specific rules on how the buildings look and are laid out. These strict and unambiquous rules differ greatly from Atlanta.

Now that we have the two different types of languages defined, there are three aspects of a language that make up a language (in a non-linguistic way).

### Ambiguity

Formal languages, as we have state, are meant to be nearly or completely unambiquous. This means that every statement has only one meaning.

### Redundancy

Formal languages are very concise in their actual meaning. This means that they employ very limited redundancy.

### Literalness

Formal languages mean exactly what they say. This is important when we talk about errors, specifically user errors.

We will dive much deeper into languages at another time, including how they are actually constructured on a general level, but this should be enough to get you going.

## What about Binary

I remember when I was in college, and I saw my first digital binary clock. I was hanging with some friends at Georgia Tech, and I thought it was one of the coolest things I have ever seen. I felt like I was in every 90's / early 2000's hacker movie at once. But I didn't actually know what binary was. I didn't understand how to read it. It felt like magic to me. So here I'm going to brake expose the truth behind that magic.

Binary is nothing more than a way for use to represent data. At the lowest level of a computer, we have binary. It is a numeral system in which we represent data using 0's and 1's. That's is cool and all, but it doesn't really tell us how this works.

In binary, as stated above, we have just two digits, which are at the power of two for each place value:

|16|8|4|2|1|
|--|--|--|--|--|
|0|0|0|0|0|

The above binary would equal to 0, as we have not assigned a value to anything. Now, let's say that we want to assign 1 to the first two places:

|16|8|4|2|1|
|--|--|--|--|--|
|0|0|0|1|1|

This would equal to 3, as we are assigning a value to the first place that has the power of

$$2^0$$

which is equal to 1. The second value that we assign a value to is

$$2^1$$

which is equal to 2, and so on. So by assigning a value to the first two values, we get our result of 3:

$$2^0 + 2^1 = 1 + 2 = 3$$

So we can look at our binary table as so:

|4|3|2|1|0|
|--|--|--|--|--|
|16|8|4|2|1|
|0|0|0|1|1|

With the top row being the exponent itself, and the second role being the value. We are going to keep it as the original view, but I want you to get this understanding down.

Let's look at another example:

|16|8|4|2|1|
|--|--|--|--|--|
|1|0|1|0|0|

Take a second to figure out the above before you look at the solution.

So let's go ahead and break this down. We see that we have assigned a value of

$$2^2$$

as well as

$$2^4$$

This would be 4 and 16 respectively. From here we simply add them together to get 20.

Now that we know how to calculate the value of our bianry, atleast in a simple form, let's look at how.

Let's get the obvious out of the way. We power our computers through electricity. But what might not be common knowledge is that within our computers, we have millions or billions of switches. We call these switches transistors. We will eventually dive into these as well when we talk about EE (electrical engineering) later on. We store our electricity into these transistors. Now, each of these transitors represent a bit with an off and on state. When we run electricity through these, we switch the states. So we can use these binary states to represent our, well, binary.

Now the last thing we are going to talk about with binary is the idea of bits and bytes. This is pretty simple, as every byte equals 8 bits. 8 bytes equal 64 bits. Furthermore, a byte is our smallest unit of addressable memory in our computer. A single byte can store one character. When we go into the size of data types, we will get a much better look at this.

## Let's Represent This Data

Now that we have an idea of how binary works, we want to look at how we actually represent this data with bytes. Previously we stated that a single byte can store one character, so let's first look at what a character is. A character is a single visual object which we use to represnt text, numbers, or even symbols. We make use of something called mapping to map a number to these characters. This is where the idea of encoding and decoding comes into play. For now, we will focus on the standard ASCII mapping. There are other mappings, such as UTF-8, and we can look at those at another time if we need to.

In ASCII, we use the number 65 for the letter A, 66 for B, and so on. The mapping also includes punctuation, as well as some other symbols. But it does not include letters that contain accent marks, or some languages with a different alphabet.

What about different data types? How do we represent things like images? Well, an image is actually broken down into pixels. Each of these pixels are actually represented in binary using the rgb system (Red, Green, and Blue system). By mixing together different strengths of Red, Green, and Blue we are able to create the colors that we see.

We can even represent things like sound and music using binary as well, though maybe we can touch on that later. As we as this, movies are just a series of pictures played quickly in succession.

## Final Notes

The aim of this section is to really get you somewhat up to speed over computer science. The aim isn't for you to come away from this section knowing how to program, or having some intimate knowledge over all things computer science. If you leave a little bit more confused, that is fine also. The main goal is to get you interested in this topic, and give you just enough of a peek to keep you coming back.

In the next section, we are going to actually start learning our first programming language, C. I will say this before we move on. I know that a lot of courses try to puch Python onto you first, or try to convince you that you can learn to program in 24 hours, or 3 months. Neither of those are true. It will take a lot of time and effort to perfect the craft. To learn what you need to in order to create your application, your product, and to keep your customers coming back.

So let's go ahead and dive in.
